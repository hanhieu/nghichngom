{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1fab1c-dd70-4a84-9e07-61baa36b72ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8313330-f5b0-4d0f-b9e5-68c6612035d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import sys\n",
    "import re\n",
    "import requests \n",
    "from bs4 import  BeautifulSoup\n",
    "import json\n",
    "from datetime import datetime\n",
    "from tqdm import trange,tnrange, tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d5d7f40-4d94-4d35-9d98-d0bf7fe08d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "from datetime import date\n",
    "from dateutil.relativedelta import relativedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a15c86af-4faa-4b85-a9f7-36f2e06c4de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pd.IndexSlice\n",
    "pd.options.display.float_format = '{:,.3f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d913e5-8651-4ca6-ab04-ec56ce46c614",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# GET SEC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc326d3-df9c-461d-b57e-12fe6b64e86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sec_data(cik, doc_type, start=0, count=60):\n",
    "    newest_pricing_date = pd.to_datetime('2018-08-01')\n",
    "    rss_url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcompany' \\\n",
    "        '&CIK={}&type={}&start={}&count={}&owner=exclude&output=atom' \\\n",
    "        .format(cik, doc_type, start, count)\n",
    "    sec_data = sec_api.get(rss_url)\n",
    "    return sec_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc21b5-3909-47f8-bc4a-7342d74f2d48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183c9a32-d5e9-4634-b8c4-6118b448ef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.sec.gov/Archives/edgar/data/320193/000032019318000145/0000320193-18-000145.txt')\n",
    "raw_10k = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661243e-2bdb-4227-b3e5-1e462f011637",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_10k"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeec054-c5a9-44e3-b766-93b6e0c24727",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# INDUSTRIAL PARK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea383ff3-3cd2-41b9-b294-bed125d81427",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Replace the URL with the webpage you want to scrape\n",
    "url = 'https://idpvn.com/en/industrial-park-in-vietnam/'\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response in the 'response' variable\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200 indicates success)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table element you want to extract (modify the selector accordingly)\n",
    "    table = soup.find('table')\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "x = pd.read_html(str(table))[0]\n",
    "x.to_excel('list_industrial.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7beaa67-e609-4374-a436-71b8826f17ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6429296-8b97-4333-8152-e6d2e5693be1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# PNG TO PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c9bb41-42c3-4bb1-a4b6-0d4b68553b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.lib.utils import ImageReader\n",
    "\n",
    "def merge_images_in_folder_to_pdf(folder_path, output_pdf_path):\n",
    "    image_paths = [os.path.join(folder_path, file) for file in os.listdir(folder_path) if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n",
    "\n",
    "    if not image_paths:\n",
    "        print(\"No images found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    c = canvas.Canvas(output_pdf_path, pagesize=letter)\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        img = ImageReader(image_path)\n",
    "        img_width, img_height = img.getSize()\n",
    "\n",
    "        c.setPageSize((img_width, img_height))\n",
    "        c.drawImage(img, 0, 0, width=img_width, height=img_height)\n",
    "        c.showPage()\n",
    "\n",
    "    c.save()\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"C:/Users/HP PAVILION/Desktop/TGN/New folder\"\n",
    "output_pdf_path = \"C:/Users/HP PAVILION/Desktop/TGN/New folder/merged_images.pdf\"\n",
    "\n",
    "merge_images_in_folder_to_pdf(folder_path, output_pdf_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a517b-abde-4f44-b8b1-78ebad27255d",
   "metadata": {},
   "source": [
    "# MONRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50601551-2fe8-4bca-b778-8dc17ee596d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import quote\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383d0f63-7f58-4395-8d72-64170c883422",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pdf_from_url(\n",
    "    pdf_url, save_path):\n",
    "    # Fetch PDF content from the URL\n",
    "    response = requests.get(pdf_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Save the PDF locally\n",
    "        with open(save_path, \"wb\") as pdf_file:\n",
    "            pdf_file.write(response.content)\n",
    "        print(f\"PDF saved successfully to {save_path}\")\n",
    "    else:\n",
    "        print(f\"Failed to fetch PDF from the URL. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a929408-3c1f-47b8-91cf-08704d283d2c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### THAM VẤN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be01fcc7-ebad-4bb2-9a6e-03fbce235805",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_link = []\n",
    "list_name = []\n",
    "for i in tnrange(1300,1500):\n",
    "    url = \"https://thamvan.monre.gov.vn/XemChiTiet/XemChiTiet\"\n",
    "    data = {\n",
    "        \"id\": i\n",
    "    }\n",
    "    try: \n",
    "        response = requests.post(url, data=data)\n",
    "        filename = response.json().get('hs')[0]['Path']\n",
    "        name = filename.rsplit('/', 1)[-1]\n",
    "        file = \"https://thamvan.monre.gov.vn/\" + str(filename)\n",
    "        file2 = file.replace(\" \", \"%20\")\n",
    "        list_link.append(file2)\n",
    "        list_name.append(name)\n",
    "    except:\n",
    "        print(\"Lỗi ở:\"+str(i))\n",
    "x = pd.DataFrame([list_name,list_link])\n",
    "x = x.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8251d6c8-e449-44be-9efb-299a19f77a63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desktop_path = os.path.expanduser(\"~/Desktop/MONRE/\")\n",
    "for i in tnrange(len(x)):\n",
    "    save_path = os.path.join(desktop_path, x.loc[i,0])\n",
    "    save_pdf_from_url(x.loc[i,1],save_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37944e52-7453-4fdc-b9a0-02be43d78da2",
   "metadata": {},
   "source": [
    "### ĐỀ XUẤT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f0e3b-dcbc-4846-b00c-acd058912b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "list_link = []\n",
    "list_name = []\n",
    "for i in tnrange(1600,1700):\n",
    "    url = \"https://monre.gov.vn/VanBan/Pages/ChiTietVanBanDuThao.aspx?pID=\"+str(i)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        pdf_links = soup.find_all('a', href=re.compile(r'\\.pdf$', re.IGNORECASE))\n",
    "        filelink = quote(pdf_links[-1].get('href'),safe = \":/\")\n",
    "        filename = pdf_links[-1].get('title')\n",
    "        list_link.append(filelink)\n",
    "        list_name.append(filename)\n",
    "    except:\n",
    "        print(\"Lỗi ở: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f687bc0-5995-4ae3-9eb3-98b0e7ad70a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = pd.DataFrame([list_name,list_link])\n",
    "x = x.T\n",
    "x = x.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee324e-a255-4d82-a074-d983cdc13eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "desktop_path = os.path.expanduser(\"~/Desktop/MONRE/\")\n",
    "for i in tnrange(len(x)):\n",
    "    save_path = os.path.join(desktop_path, x.iloc[i,0])\n",
    "    save_pdf_from_url(x.iloc[i,1],save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b32b0d-fa6c-4bda-9713-9c03fbf795b9",
   "metadata": {},
   "source": [
    "# INDUSTRIAL PARK FROM IDPVN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f709fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# using BeautifulSoup and requests libraries to scrape the table from the website \"https://idpvn.com/?p=2208\" and cookied the website to get the full table\n",
    "# Send an HTTP GET request to the URL and store the response in the 'response' variable\n",
    "response = requests.get('https://idpvn.com/?p=2208')\n",
    "\n",
    "# Check if the request was successful (status code 200 indicates success)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table element you want to extract (modify the selector accordingly)\n",
    "    table = soup.find('table')\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# transfer this table to dataframe\n",
    "df = pd.read_html(str(table))[0]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a426b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html5lib \n",
    "import pandas as pd\n",
    "\n",
    "# Send an HTTP GET request to the URL and store the response in the 'response' variable\n",
    "response = requests.get(\"https://idpvn.com/khu-cong-nghiep/\")\n",
    "\n",
    "# Check if the request was successful (status code 200 indicates success)\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find the table element you want to extract (modify the selector accordingly) and get href link of each element in that table\n",
    "    table = soup.find('table')\n",
    "    links = []\n",
    "    for link in table.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "else:\n",
    "    print(\"Failed to retrieve the webpage. Status code:\", response.status_code)\n",
    "\n",
    "# convert links to dataframe, with another column is the name of each link\n",
    "df = pd.DataFrame(links)\n",
    "df.columns = ['links']\n",
    "df['name'] = df['links'].str.split('/').str[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9148a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730d9ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a loop to get the table from each link in df and store it in 1 dataframe, with tnrange to show the progress, and use try except in case no tables found in the link\n",
    "list_df = []\n",
    "for i in tnrange(len(df)):\n",
    "    try:\n",
    "        url = df['links'][i]\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        df2 = pd.read_html(str(table))[0]\n",
    "        df2['name'] = df['name'][i]\n",
    "        list_df.append(df2)\n",
    "    except:\n",
    "        print(\"Lỗi ở: \" + df['name'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44582bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list_df to a dataframe\n",
    "df3 = pd.concat(list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in df4 as a copy of df3, merge  with 3 columns Tên Công Ty, Tên Công ty, Tên as 1 column name \"Company\"\n",
    "df4 = df3.copy()\n",
    "df4['Company'] = df4['Tên Công Ty'].fillna(df4['Tên Công ty']).fillna(df4['TEN'])\n",
    "# create column \"Province\" by spliting the second last \"/\" in  \"links\" column in df \n",
    "df['Province'] = df['links'].str.split('/').str[-3]\n",
    "# join df4 with df on column \"name\" to get column \"Province\" in df4\n",
    "df4 = df4.join(df.set_index('name'), on='name')\n",
    "# df4 = df4.drop(columns = ['Tên Công Ty','Tên Công ty','TEN'])\n",
    "#rearrange the columns in df4, with 1st column is Company, 2nd column is name, 3rd column is Loại Hình, 4th column is Ngành Nghề Chính, 5th column is Năm Thành Lập \n",
    "df4 = df4[['Province','name','Company','Loại Hình','Ngành Nghề Chính','Năm Thành Lập']]\n",
    "df4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b115a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save df4 to excel file in desktop\n",
    "desktop_path = os.path.expanduser(\"~/Desktop/\")\n",
    "df4.to_excel(desktop_path + 'list_industrial.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d7d266",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e506249",
   "metadata": {},
   "source": [
    "# BỘ XÂY DỰNG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a698ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP PAVILION\\AppData\\Local\\Temp\\ipykernel_38940\\307870266.py:4: TqdmDeprecationWarning: Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\n",
      "  for i in tnrange(1,500):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cebeff2e684c40359235f0cdeb4e8411",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/499 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# crawl data from website \"https://batdongsan.xaydung.gov.vn/ThongBao.aspx?vID=\" with vID from 1 to 1000 \n",
    "list_link = []\n",
    "list_name = []\n",
    "for i in tnrange(1,500):\n",
    "    url = \"https://batdongsan.xaydung.gov.vn/ThongBao.aspx?vID=\"+str(i)\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text = soup.find(class_=\"tieude\")\n",
    "        # clean \\r\\n and all blank space in text\n",
    "        text = text.text.replace('\\r\\n','').strip()\n",
    "        list_link.append(str(i))\n",
    "        list_name.append(text)\n",
    "\n",
    "    except:\n",
    "        print(\"Lỗi ở: \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a93493e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe with 2 columns: link and name from list_link and list_name\n",
    "x = pd.DataFrame([list_name,list_link]).T\n",
    "x.columns = ['name','link']\n",
    "x.to_excel('moc.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cba511a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find path of x\n",
    "import os\n",
    "desktop_path = os.path.expanduser(\"~/Desktop/\")\n",
    "x.to_excel(desktop_path + 'moc.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fbd0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
